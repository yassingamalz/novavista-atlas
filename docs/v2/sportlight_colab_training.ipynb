{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Sportlight HRNet Training - Google Colab\n",
    "\n",
    "**NovaVista Atlas v2 - Egyptian League Analytics**\n",
    "\n",
    "This notebook trains the Sportlight HRNet model for soccer field keypoint detection.\n",
    "\n",
    "**Original Paper:** SoccerNet Camera Calibration Challenge 2023 (1st Place)\n",
    "\n",
    "**Performance:** 73.22% Accuracy, 75.59% Completeness\n",
    "\n",
    "**GPU Required:** T4 (16GB) - Available on Free Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Setup Checklist\n",
    "\n",
    "Before running:\n",
    "- [ ] Upload SoccerNet dataset to Google Drive\n",
    "- [ ] Organize as: `MyDrive/soccernet_dataset/train`, `valid`, `test`\n",
    "- [ ] Each folder should have paired `.jpg` and `.json` files\n",
    "- [ ] Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU (T4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "print(\"üì¶ Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úÖ Drive mounted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Update this path to your dataset location\n",
    "DATASET_BASE = \"/content/drive/MyDrive/soccernet_dataset\"\n",
    "\n",
    "print(\"üîç Checking dataset...\\n\")\n",
    "\n",
    "for split in ['train', 'valid']:\n",
    "    split_path = Path(DATASET_BASE) / split\n",
    "    \n",
    "    if not split_path.exists():\n",
    "        print(f\"‚ùå {split} folder not found at {split_path}\")\n",
    "        continue\n",
    "    \n",
    "    jpg_files = list(split_path.glob('*.jpg'))\n",
    "    json_files = list(split_path.glob('*.json'))\n",
    "    \n",
    "    print(f\"üìÇ {split}:\")\n",
    "    print(f\"   Images: {len(jpg_files)}\")\n",
    "    print(f\"   JSONs: {len(json_files)}\")\n",
    "    \n",
    "    # Check first JSON structure\n",
    "    if json_files:\n",
    "        sample_json = json_files[0]\n",
    "        with open(sample_json) as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"   Sample JSON keys: {list(data.keys())}\")\n",
    "    print()\n",
    "\n",
    "print(\"‚úÖ Dataset verification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Clone Sportlight Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NikolasEnt/soccernet-calibration-sportlight.git\n",
    "%cd soccernet-calibration-sportlight\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è Installing dependencies...\\n\")\n",
    "\n",
    "!pip install -q torch torchvision\n",
    "!pip install -q opencv-python\n",
    "!pip install -q hydra-core\n",
    "!pip install -q argus-learn\n",
    "!pip install -q omegaconf\n",
    "!pip install -q albumentations\n",
    "!pip install -q scipy\n",
    "!pip install -q scikit-image\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Setup Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"üìÇ Setting up data directories...\\n\")\n",
    "\n",
    "# Create workdir structure\n",
    "os.makedirs(\"/workdir/data/dataset\", exist_ok=True)\n",
    "os.makedirs(\"/workdir/data/experiments\", exist_ok=True)\n",
    "\n",
    "# Create symbolic links to dataset\n",
    "!ln -sf {DATASET_BASE}/train /workdir/data/dataset/train\n",
    "!ln -sf {DATASET_BASE}/valid /workdir/data/dataset/valid\n",
    "\n",
    "# Verify links\n",
    "!ls -la /workdir/data/dataset/\n",
    "\n",
    "print(\"\\n‚úÖ Data directories ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Modify Training Config for Colab\n",
    "\n",
    "**Adjustments:**\n",
    "- Batch size: 8 ‚Üí 4 (fits in 16GB GPU)\n",
    "- Input size: 960√ó540 ‚Üí 720√ó405 (reduce memory)\n",
    "- Workers: 8 ‚Üí 2 (Colab CPU limitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîß Modifying training config for Colab...\\n\")\n",
    "\n",
    "config_path = \"src/models/hrnet/train_config.yaml\"\n",
    "\n",
    "# Read config\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Show original settings\n",
    "print(\"üìã Original settings:\")\n",
    "print(f\"   Batch size: {config['data_params']['batch_size']}\")\n",
    "print(f\"   Input size: {config['data_params']['input_size']}\")\n",
    "print(f\"   Workers: {config['data_params']['num_workers']}\")\n",
    "\n",
    "# Modify for Colab\n",
    "config['data_params']['batch_size'] = 4  # Reduced from 8\n",
    "config['data_params']['input_size'] = [720, 405]  # Reduced from [960, 540]\n",
    "config['data_params']['num_workers'] = 2  # Reduced from 8\n",
    "\n",
    "# Adjust prediction sizes accordingly\n",
    "config['model']['params']['loss']['pred_size'] = [203, 360]  # 405/2, 720/2\n",
    "config['model']['params']['prediction_transform']['size'] = [405, 720]\n",
    "\n",
    "# Enable AMP (Automatic Mixed Precision) for memory efficiency\n",
    "config['model']['params']['amp'] = True\n",
    "\n",
    "# Save modified config\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(\"\\n‚úÖ Modified settings:\")\n",
    "print(f\"   Batch size: {config['data_params']['batch_size']}\")\n",
    "print(f\"   Input size: {config['data_params']['input_size']}\")\n",
    "print(f\"   Workers: {config['data_params']['num_workers']}\")\n",
    "print(\"\\n‚úÖ Config updated for Colab!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"üîç GPU Information:\\n\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"\\n‚úÖ Ready to train!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No GPU found!\")\n",
    "    print(\"Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Start Training\n",
    "\n",
    "**Expected Duration:** 6-10 hours on T4 GPU\n",
    "\n",
    "**Training will:**\n",
    "- Run for max 200 epochs\n",
    "- Early stopping after 32 epochs without improvement\n",
    "- Save checkpoints every 2 epochs\n",
    "- Save best model based on validation loss\n",
    "\n",
    "**Note:** Keep this tab open or use Colab Pro for background execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting training...\")\n",
    "print(\"‚è∞ Expected time: 6-10 hours\")\n",
    "print(\"üìä Monitor progress below\\n\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "!python src/models/hrnet/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ View Training Progress (Optional)\n",
    "\n",
    "Run this in a separate cell while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View latest log file\n",
    "import glob\n",
    "\n",
    "log_files = glob.glob(\"/workdir/data/experiments/*/log.txt\")\n",
    "if log_files:\n",
    "    latest_log = sorted(log_files)[-1]\n",
    "    print(f\"üìä Reading: {latest_log}\\n\")\n",
    "    !tail -n 50 {latest_log}\n",
    "else:\n",
    "    print(\"No log files found yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Find Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîç Looking for trained models...\\n\")\n",
    "\n",
    "# Find experiment folder\n",
    "exp_folders = glob.glob(\"/workdir/data/experiments/HRNet_57_*\")\n",
    "\n",
    "if not exp_folders:\n",
    "    print(\"‚ùå No experiment folders found\")\n",
    "else:\n",
    "    exp_folder = exp_folders[0]\n",
    "    print(f\"üìÇ Experiment: {exp_folder}\\n\")\n",
    "    \n",
    "    # Find all models\n",
    "    all_models = glob.glob(f\"{exp_folder}/*.pth\")\n",
    "    \n",
    "    if all_models:\n",
    "        print(f\"üì¶ Found {len(all_models)} model(s):\\n\")\n",
    "        \n",
    "        # Find best EvalAI model\n",
    "        evalai_models = [m for m in all_models if 'evalai' in m]\n",
    "        if evalai_models:\n",
    "            best_model = sorted(evalai_models)[-1]\n",
    "            print(f\"‚úÖ Best model: {Path(best_model).name}\")\n",
    "            print(f\"   Full path: {best_model}\")\n",
    "            \n",
    "            # Store for next cell\n",
    "            BEST_MODEL_PATH = best_model\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No 'evalai' models found, showing all:\")\n",
    "            for model in all_models:\n",
    "                print(f\"   - {Path(model).name}\")\n",
    "    else:\n",
    "        print(\"‚ùå No models found in experiment folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üíæ Saving trained model...\\n\")\n",
    "\n",
    "try:\n",
    "    # Copy to Google Drive\n",
    "    drive_save_path = \"/content/drive/MyDrive/sportlight_models\"\n",
    "    os.makedirs(drive_save_path, exist_ok=True)\n",
    "    \n",
    "    model_name = Path(BEST_MODEL_PATH).name\n",
    "    drive_model_path = f\"{drive_save_path}/{model_name}\"\n",
    "    \n",
    "    shutil.copy(BEST_MODEL_PATH, drive_model_path)\n",
    "    print(f\"‚úÖ Saved to Google Drive: {drive_model_path}\")\n",
    "    \n",
    "    # Also download to local computer\n",
    "    print(\"\\nüì• Downloading to your computer...\")\n",
    "    files.download(BEST_MODEL_PATH)\n",
    "    \n",
    "    print(\"\\n‚úÖ Model saved successfully!\")\n",
    "    print(f\"\\nüìä Model info:\")\n",
    "    print(f\"   Name: {model_name}\")\n",
    "    print(f\"   Size: {Path(BEST_MODEL_PATH).stat().st_size / 1e6:.1f} MB\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"‚ùå No model found to save. Training may not have completed.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà Training Summary\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Read log file\n",
    "    log_path = glob.glob(\"/workdir/data/experiments/*/log.txt\")[0]\n",
    "    with open(log_path) as f:\n",
    "        log_lines = f.readlines()\n",
    "    \n",
    "    # Extract key metrics from last few lines\n",
    "    print(\"Last 10 log entries:\\n\")\n",
    "    for line in log_lines[-10:]:\n",
    "        print(line.strip())\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"\\n‚úÖ Training complete!\")\n",
    "    print(\"\\nüìã Next steps:\")\n",
    "    print(\"1. Test model on Egyptian League frames\")\n",
    "    print(\"2. Integrate into Atlas v2 pipeline\")\n",
    "    print(\"3. Validate performance metrics\")\n",
    "    \n",
    "except:\n",
    "    print(\"No training logs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üÜò Troubleshooting\n",
    "\n",
    "### Out of Memory Error\n",
    "\n",
    "If you get `CUDA out of memory`, run this cell to further reduce batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emergency memory reduction\n",
    "config_path = \"src/models/hrnet/train_config.yaml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Further reduce\n",
    "config['data_params']['batch_size'] = 2  # From 4 to 2\n",
    "config['data_params']['input_size'] = [640, 360]  # From [720, 405]\n",
    "config['model']['params']['loss']['pred_size'] = [180, 320]\n",
    "config['model']['params']['prediction_transform']['size'] = [360, 640]\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(\"‚úÖ Config reduced for lower memory usage\")\n",
    "print(\"‚ö†Ô∏è Training will be slower but more stable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session Timeout\n",
    "\n",
    "**Free Colab timeout:** 12 hours\n",
    "\n",
    "**Solutions:**\n",
    "1. Use Colab Pro ($10/month) for 24-hour sessions\n",
    "2. Resume from checkpoint if interrupted\n",
    "3. Train in multiple sessions\n",
    "\n",
    "**To resume training:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume from last checkpoint\n",
    "import glob\n",
    "\n",
    "checkpoints = glob.glob(\"/workdir/data/experiments/*/save-*.pth\")\n",
    "if checkpoints:\n",
    "    latest_checkpoint = sorted(checkpoints)[-1]\n",
    "    print(f\"Found checkpoint: {latest_checkpoint}\")\n",
    "    \n",
    "    # Update config to use checkpoint\n",
    "    config_path = \"src/models/hrnet/train_config.yaml\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    config['model']['params']['pretrain'] = latest_checkpoint\n",
    "    \n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "    \n",
    "    print(\"‚úÖ Config updated to resume from checkpoint\")\n",
    "    print(\"Run training cell again to continue\")\n",
    "else:\n",
    "    print(\"No checkpoints found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Training Complete!\n",
    "\n",
    "**What you have now:**\n",
    "- Trained HRNet keypoint detection model (~200 MB)\n",
    "- Model saved in Google Drive and downloaded locally\n",
    "- Ready for Phase 2: Testing on Egyptian League\n",
    "\n",
    "**Next steps:**\n",
    "1. Test on Egyptian League frames (see `PHASE_2_TESTING.md`)\n",
    "2. Integrate into Atlas v2 pipeline\n",
    "3. Production validation\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook:** `sportlight_colab_training.ipynb`\n",
    "\n",
    "**Author:** NovaVista Atlas Team\n",
    "\n",
    "**Date:** October 2025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
