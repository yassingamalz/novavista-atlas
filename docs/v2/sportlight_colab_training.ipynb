{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Sportlight HRNet Training - Google Colab\n",
    "\n",
    "**NovaVista Atlas v2 - Egyptian League Analytics**\n",
    "\n",
    "This notebook trains the Sportlight HRNet model for soccer field keypoint detection.\n",
    "\n",
    "**Original Paper:** SoccerNet Camera Calibration Challenge 2023 (1st Place)\n",
    "\n",
    "**Performance:** 73.22% Accuracy, 75.59% Completeness\n",
    "\n",
    "**Dataset:** SoccerNet Camera Calibration (downloaded automatically)\n",
    "\n",
    "**GPU Required:** T4 (16GB) - Available on Free Colab\n",
    "\n",
    "---\n",
    "\n",
    "**Training Optimizations Built-in:**\n",
    "- ‚úÖ Mixed Precision (AMP) - Faster + less memory\n",
    "- ‚úÖ Early Stopping - 32 epochs patience\n",
    "- ‚úÖ Auto Checkpoints - Every 2 epochs\n",
    "- ‚úÖ Best Model Saving - Val loss + metrics\n",
    "- ‚úÖ Memory Optimized - Fits T4 16GB GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Setup Checklist\n",
    "\n",
    "Before running:\n",
    "- [ ] Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\n",
    "- [ ] Run all cells in order\n",
    "- [ ] Dataset will be downloaded automatically (~5-10 GB)\n",
    "- [ ] Training takes 6-10 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"üîç GPU Information:\\n\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"\\n‚úÖ GPU ready for training!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No GPU found!\")\n",
    "    print(\"Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Clone Sportlight Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì• Cloning Sportlight repository...\\n\")\n",
    "\n",
    "!git clone https://github.com/NikolasEnt/soccernet-calibration-sportlight.git\n",
    "%cd soccernet-calibration-sportlight\n",
    "\n",
    "print(\"\\n‚úÖ Repository cloned!\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è Installing dependencies...\\n\")\n",
    "\n",
    "# Core ML libraries\n",
    "!pip install -q torch torchvision\n",
    "!pip install -q opencv-python\n",
    "!pip install -q hydra-core\n",
    "!pip install -q omegaconf\n",
    "!pip install -q albumentations\n",
    "!pip install -q scipy\n",
    "!pip install lsq-ellipse\n",
    "!pip install -q scikit-image\n",
    "\n",
    "# Argus framework (install from GitHub)\n",
    "!pip install -q git+https://github.com/lRomul/argus.git\n",
    "\n",
    "# SoccerNet API for dataset download\n",
    "!pip install -q SoccerNet\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Download SoccerNet Dataset\n",
    "\n",
    "**This will download the official SoccerNet Camera Calibration dataset:**\n",
    "- Training set\n",
    "- Validation set\n",
    "- Test set\n",
    "\n",
    "**Size:** ~5-10 GB\n",
    "\n",
    "**Time:** 10-20 minutes depending on connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SoccerNet.Downloader import SoccerNetDownloader\n",
    "import os\n",
    "\n",
    "print(\"üì¶ Downloading SoccerNet Camera Calibration dataset...\")\n",
    "print(\"‚è∞ This may take 10-20 minutes\\n\")\n",
    "\n",
    "# Create dataset directory\n",
    "dataset_dir = \"/content/soccernet_data\"\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Initialize downloader\n",
    "downloader = SoccerNetDownloader(LocalDirectory=dataset_dir)\n",
    "\n",
    "# Download camera calibration data (correct task name)\n",
    "print(\"Downloading calibration-2023 dataset...\")\n",
    "downloader.downloadDataTask(\n",
    "    task=\"calibration-2023\",\n",
    "    split=[\"train\", \"valid\", \"test\"]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Dataset downloaded!\")\n",
    "\n",
    "# Verify download\n",
    "print(\"\\nüìä Dataset structure:\")\n",
    "!ls -la {dataset_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5Ô∏è‚É£ Extract Dataset Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_BASE = \"/content/soccernet_data/calibration-2023\"\n",
    "dataset_path = Path(DATASET_BASE)\n",
    "\n",
    "print(\"üì¶ Extracting dataset archives...\\n\")\n",
    "\n",
    "# Find all zip files\n",
    "zip_files = list(dataset_path.glob(\"*.zip\"))\n",
    "\n",
    "if not zip_files:\n",
    "    print(\"‚ö†Ô∏è No zip files found. Dataset may already be extracted.\")\n",
    "else:\n",
    "    print(f\"Found {len(zip_files)} archives to extract\\n\")\n",
    "    \n",
    "    for zip_path in zip_files:\n",
    "        print(f\"üìÇ Extracting {zip_path.name}...\")\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(dataset_path)\n",
    "        \n",
    "        print(f\"   ‚úÖ Done\")\n",
    "    \n",
    "    print(\"\\n‚úÖ All archives extracted!\")\n",
    "\n",
    "# Show extracted structure\n",
    "print(\"\\nüìä Extracted structure:\")\n",
    "!ls -la {DATASET_BASE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Verify Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_BASE = \"/content/soccernet_data/calibration-2023\"\n",
    "\n",
    "print(\"üîç Verifying dataset...\\n\")\n",
    "\n",
    "for split in ['train', 'valid']:\n",
    "    split_path = Path(DATASET_BASE) / split\n",
    "    \n",
    "    if not split_path.exists():\n",
    "        print(f\"‚ö†Ô∏è {split} folder not found at {split_path}\")\n",
    "        continue\n",
    "    \n",
    "    jpg_files = list(split_path.glob('**/*.jpg'))\n",
    "    json_files = list(split_path.glob('**/*.json'))\n",
    "    \n",
    "    print(f\"üìÇ {split}:\")\n",
    "    print(f\"   Images: {len(jpg_files)}\")\n",
    "    print(f\"   JSONs: {len(json_files)}\")\n",
    "    \n",
    "    # Check first JSON structure\n",
    "    if json_files:\n",
    "        sample_json = json_files[0]\n",
    "        try:\n",
    "            with open(sample_json) as f:\n",
    "                data = json.load(f)\n",
    "            print(f\"   Sample JSON keys: {list(data.keys())}\")\n",
    "            print(f\"   Sample file: {sample_json.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Error reading JSON: {e}\")\n",
    "    print()\n",
    "\n",
    "print(\"‚úÖ Dataset verification complete!\")\n",
    "print(f\"\\nüìç Dataset location: {DATASET_BASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Setup Data Directories for Sportlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"üìÇ Setting up Sportlight data directories...\\n\")\n",
    "\n",
    "# Create workdir structure expected by Sportlight\n",
    "os.makedirs(\"/workdir/data/dataset\", exist_ok=True)\n",
    "os.makedirs(\"/workdir/data/experiments\", exist_ok=True)\n",
    "\n",
    "# Create symbolic links to dataset\n",
    "# Update DATASET_BASE if needed from previous cell\n",
    "!ln -sf {DATASET_BASE}/train /workdir/data/dataset/train\n",
    "!ln -sf {DATASET_BASE}/valid /workdir/data/dataset/valid\n",
    "\n",
    "# Verify links\n",
    "print(\"Created symbolic links:\")\n",
    "!ls -la /workdir/data/dataset/\n",
    "\n",
    "# Test access\n",
    "train_files = !ls /workdir/data/dataset/train | head -5\n",
    "print(f\"\\nSample train files:\")\n",
    "for f in train_files:\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Data directories ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Modify Training Config for Colab\n",
    "\n",
    "**Adjustments for 16GB T4 GPU:**\n",
    "- Batch size: 8 ‚Üí 4 (fits in 16GB)\n",
    "- Input size: 960√ó540 ‚Üí 720√ó405 (reduce memory)\n",
    "- Workers: 8 ‚Üí 2 (Colab CPU limitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîß Modifying training config for Colab...\\n\")\n",
    "\n",
    "config_path = \"src/models/hrnet/train_config.yaml\"\n",
    "\n",
    "# Read config\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Show original settings\n",
    "print(\"üìã Original settings:\")\n",
    "print(f\"   Batch size: {config['data_params']['batch_size']}\")\n",
    "print(f\"   Input size: {config['data_params']['input_size']}\")\n",
    "print(f\"   Workers: {config['data_params']['num_workers']}\")\n",
    "\n",
    "# Modify for Colab (16GB T4)\n",
    "config['data_params']['batch_size'] = 4  # Reduced from 8\n",
    "config['data_params']['input_size'] = [720, 405]  # Reduced from [960, 540]\n",
    "config['data_params']['num_workers'] = 2  # Reduced from 8\n",
    "\n",
    "# Adjust prediction sizes accordingly\n",
    "config['model']['params']['loss']['pred_size'] = [203, 360]  # 405/2, 720/2\n",
    "config['model']['params']['prediction_transform']['size'] = [405, 720]\n",
    "\n",
    "# Performance optimizations\n",
    "config['model']['params']['amp'] = True  # AMP for faster training + less memory\n",
    "\n",
    "# Early stopping already configured (32 epochs patience)\n",
    "# Checkpoints save every 2 epochs automatically\n",
    "# Best models saved based on validation metrics\n",
    "\n",
    "# Save modified config\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(\"\\n‚úÖ Modified settings:\")\n",
    "print(f\"   Batch size: {config['data_params']['batch_size']}\")\n",
    "print(f\"   Input size: {config['data_params']['input_size']}\")\n",
    "print(f\"   Workers: {config['data_params']['num_workers']}\")\n",
    "print(f\"   AMP enabled: {config['model']['params']['amp']}\")\n",
    "print(\"\\n‚úÖ Config optimized for Colab T4 GPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ (Optional) Quick Test Run\n",
    "\n",
    "**Test the pipeline first with 5 epochs (~30 minutes)**\n",
    "\n",
    "Skip this if you want to start full training immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick 5-epoch test to validate setup\n",
    "import yaml\n",
    "\n",
    "config_path = \"src/models/hrnet/train_config.yaml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Test mode: 5 epochs\n",
    "config['train_params']['max_epochs'] = 5\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(\"‚ö° Quick test mode: 5 epochs (~30 min)\")\n",
    "print(\"This validates your setup before full training\")\n",
    "print(\"\\nAfter test, reset max_epochs to 200 for full training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Start Training\n",
    "\n",
    "**‚è∞ Expected Duration:** 6-10 hours on T4 GPU\n",
    "\n",
    "**Training will:**\n",
    "- Run for max 200 epochs\n",
    "- Early stopping after 32 epochs without improvement\n",
    "- Save checkpoints every 2 epochs to `/workdir/data/experiments/`\n",
    "- Save best model based on validation metrics\n",
    "\n",
    "**‚ö†Ô∏è Important:** \n",
    "- Keep this tab open (or use Colab Pro for background execution)\n",
    "- Free Colab has 12-hour limit\n",
    "- Training can be resumed from checkpoints if interrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set working directory and Python paths\n",
    "repo_path = \"/content/soccernet-calibration-sportlight\"\n",
    "os.chdir(repo_path)\n",
    "sys.path.insert(0, repo_path)\n",
    "\n",
    "# Set Hydra config path\n",
    "config_dir = f\"{repo_path}/src/models/hrnet\"\n",
    "os.environ['HYDRA_CONFIG_DIR'] = config_dir\n",
    "\n",
    "print(\"üöÄ Starting Sportlight HRNet training...\")\n",
    "print(\"‚è∞ Expected time: 6-10 hours\")\n",
    "print(\"üìä Logs will appear below\")\n",
    "print(\"üíæ Models saved to /workdir/data/experiments/\\n\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "!python -m src.models.hrnet.train --config-dir={config_dir} --config-name=train_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Find Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print(\"üîç Looking for trained models...\\n\")\n",
    "\n",
    "# Find experiment folder\n",
    "exp_folders = glob.glob(\"/workdir/data/experiments/HRNet_57_*\")\n",
    "\n",
    "if not exp_folders:\n",
    "    print(\"‚ùå No experiment folders found\")\n",
    "    print(\"Training may not have started or completed yet\")\n",
    "else:\n",
    "    exp_folder = exp_folders[0]\n",
    "    print(f\"üìÇ Experiment folder: {Path(exp_folder).name}\\n\")\n",
    "    \n",
    "    # Find all models\n",
    "    all_models = glob.glob(f\"{exp_folder}/*.pth\")\n",
    "    \n",
    "    if all_models:\n",
    "        print(f\"üì¶ Found {len(all_models)} model checkpoint(s)\\n\")\n",
    "        \n",
    "        # Find best EvalAI model (best performance on validation)\n",
    "        evalai_models = [m for m in all_models if 'evalai' in m]\n",
    "        \n",
    "        # Find best PCKs model (best keypoint accuracy)\n",
    "        pcks_models = [m for m in all_models if 'pcks' in m]\n",
    "        \n",
    "        if evalai_models:\n",
    "            best_evalai = sorted(evalai_models)[-1]\n",
    "            model_size = os.path.getsize(best_evalai) / (1024**2)\n",
    "            print(f\"‚úÖ Best EvalAI model: {Path(best_evalai).name}\")\n",
    "            print(f\"   Size: {model_size:.1f} MB\")\n",
    "            print(f\"   Path: {best_evalai}\\n\")\n",
    "            BEST_MODEL_PATH = best_evalai\n",
    "            \n",
    "        if pcks_models:\n",
    "            best_pcks = sorted(pcks_models)[-1]\n",
    "            print(f\"‚úÖ Best PCKs model: {Path(best_pcks).name}\")\n",
    "            print(f\"   Path: {best_pcks}\\n\")\n",
    "            \n",
    "        # Show all models\n",
    "        print(\"üìã All saved models:\")\n",
    "        for model in sorted(all_models):\n",
    "            model_size = os.path.getsize(model) / (1024**2)\n",
    "            print(f\"   - {Path(model).name} ({model_size:.1f} MB)\")\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ùå No models found in experiment folder\")\n",
    "        print(\"Training may still be in progress\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Download Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üíæ Downloading trained model...\\n\")\n",
    "\n",
    "try:\n",
    "    # Check if model exists\n",
    "    if 'BEST_MODEL_PATH' not in globals():\n",
    "        raise NameError(\"No model found\")\n",
    "    \n",
    "    model_name = Path(BEST_MODEL_PATH).name\n",
    "    model_size = os.path.getsize(BEST_MODEL_PATH) / (1024**2)\n",
    "    \n",
    "    print(f\"üì¶ Model: {model_name}\")\n",
    "    print(f\"üìè Size: {model_size:.1f} MB\")\n",
    "    print(f\"\\nüì• Starting download...\\n\")\n",
    "    \n",
    "    # Download to local machine\n",
    "    files.download(BEST_MODEL_PATH)\n",
    "    \n",
    "    print(\"\\n‚úÖ Model downloaded successfully!\")\n",
    "    print(\"\\nüìã Model info:\")\n",
    "    print(f\"   Filename: {model_name}\")\n",
    "    print(f\"   Size: {model_size:.1f} MB\")\n",
    "    print(f\"   Location: Your Downloads folder\")\n",
    "    \n",
    "    print(\"\\nüìù Next steps:\")\n",
    "    print(\"   1. Move model to your project: atlas/v2/models/\")\n",
    "    print(\"   2. Test on Egyptian League frames\")\n",
    "    print(\"   3. Integrate into Atlas pipeline\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"‚ùå No model found to download\")\n",
    "    print(\"Training may not have completed yet\")\n",
    "    print(\"Run the 'Find Best Model' cell first\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà Training Summary\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    # Read log file\n",
    "    log_files = glob.glob(\"/workdir/data/experiments/*/log.txt\")\n",
    "    if log_files:\n",
    "        log_path = sorted(log_files)[-1]\n",
    "        with open(log_path) as f:\n",
    "            log_lines = f.readlines()\n",
    "        \n",
    "        # Extract key metrics from last 20 lines\n",
    "        print(\"\\nüìä Final training metrics:\\n\")\n",
    "        for line in log_lines[-20:]:\n",
    "            if 'Epoch' in line or 'loss' in line or 'evalai' in line:\n",
    "                print(line.strip())\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"\\n‚úÖ Training complete!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No training logs found\")\n",
    "        \n",
    "    print(\"\\nüìã Next steps:\")\n",
    "    print(\"   1. ‚úÖ Model trained and downloaded\")\n",
    "    print(\"   2. üìä Test on Egyptian League frames (Phase 2)\")\n",
    "    print(\"   3. üîß Integrate into Atlas v2 pipeline (Phase 3)\")\n",
    "    print(\"   4. ‚úÖ Production validation (Phase 5)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading logs: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üÜò Troubleshooting\n",
    "\n",
    "### Out of Memory Error\n",
    "\n",
    "If you get `CUDA out of memory`, run this to reduce memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emergency memory reduction\n",
    "import yaml\n",
    "\n",
    "config_path = \"src/models/hrnet/train_config.yaml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"‚ö†Ô∏è Applying emergency memory reduction...\\n\")\n",
    "\n",
    "# Further reduce settings\n",
    "config['data_params']['batch_size'] = 2  # From 4 to 2\n",
    "config['data_params']['input_size'] = [640, 360]  # From [720, 405]\n",
    "config['model']['params']['loss']['pred_size'] = [180, 320]\n",
    "config['model']['params']['prediction_transform']['size'] = [360, 640]\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(\"‚úÖ Config updated:\")\n",
    "print(f\"   Batch size: 2 (was 4)\")\n",
    "print(f\"   Input size: 640√ó360 (was 720√ó405)\")\n",
    "print(\"\\n‚ö†Ô∏è Training will be slower but more stable\")\n",
    "print(\"Restart the training cell now\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume from Checkpoint\n",
    "\n",
    "If training was interrupted, resume from last checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import yaml\n",
    "\n",
    "print(\"üîç Looking for checkpoints...\\n\")\n",
    "\n",
    "checkpoints = glob.glob(\"/workdir/data/experiments/*/save-*.pth\")\n",
    "\n",
    "if checkpoints:\n",
    "    latest_checkpoint = sorted(checkpoints)[-1]\n",
    "    print(f\"‚úÖ Found checkpoint: {Path(latest_checkpoint).name}\\n\")\n",
    "    \n",
    "    # Update config to resume from checkpoint\n",
    "    config_path = \"src/models/hrnet/train_config.yaml\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    config['model']['params']['pretrain'] = latest_checkpoint\n",
    "    \n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "    \n",
    "    print(\"‚úÖ Config updated to resume from checkpoint\")\n",
    "    print(f\"   Checkpoint: {latest_checkpoint}\")\n",
    "    print(\"\\nüìù Run the training cell again to continue training\")\n",
    "else:\n",
    "    print(\"‚ùå No checkpoints found\")\n",
    "    print(\"Training must be started from scratch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Training Complete!\n",
    "\n",
    "**What you have now:**\n",
    "- ‚úÖ Trained HRNet keypoint detection model\n",
    "- ‚úÖ Model downloaded to your computer\n",
    "- ‚úÖ Ready for Phase 2: Testing on Egyptian League\n",
    "\n",
    "**Performance expectations:**\n",
    "- Keypoint accuracy: 70-75%\n",
    "- Completeness: 75-80%\n",
    "- Model size: ~200-300 MB\n",
    "\n",
    "**Next steps:**\n",
    "1. Move model to: `atlas/v2/models/sportlight_hrnet.pth`\n",
    "2. Test on 50+ Egyptian League frames\n",
    "3. Measure completeness and accuracy\n",
    "4. Integrate into Atlas v2 pipeline\n",
    "\n",
    "---\n",
    "\n",
    "**Training Method:** SoccerNet dataset (official)\n",
    "\n",
    "**Training Time:** 6-10 hours on T4\n",
    "\n",
    "**Solution:** Sportlight (SoccerNet 2023 Winner)\n",
    "\n",
    "**Date:** October 2025"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
